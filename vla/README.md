一些关于 3D Robotic 和 Vision-Language-Action (VLA) 大模型的论文选读

-----

**3D-MVP: 3D Multiview Pretraining for Robotic Manipulation**
[[arxiv](https://arxiv.org/abs/2406.18158)]
[[note](3DMVP.html)]

**3D Diffuser Actor: Policy Diffusion with 3D Scene Representations**
[[arxiv](https://arxiv.org/abs/2402.10885)]
[[note](3DDA.html)]

**3D Diffusion Policy: Generalizable Visuomotor Policy Learning via Simple 3D Representations**
[[arxiv](https://arxiv.org/abs/2403.03954)]
[[note](3DDP.html)]

**Any2Point: Empowering Any-modality Large Models for Efficient 3D Understanding**
[[arxiv](https://arxiv.org/abs/2404.07989)]
[[note](Any2Point.html)]

**Act3D: 3D Feature Field Transformers for Multi-Task Robotic Manipulation**
[[arxiv](https://arxiv.org/abs/2306.17817)]
[[note](Act3D.html)]

**Learning 3D Representations from 2D Pre-trained Models via Image-to-Point Masked Autoencoders**
[[arxiv](https://arxiv.org/abs/2212.06785)]
[[note](I2PMAE.html)]

**Point Cloud Matters: Rethinking the Impact of Different Observation Spaces on Robot Learning**
[[arxiv](https://arxiv.org/abs/2402.02500)]
[[note](OBSBench.html)]

**PolarNet: 3D Point Clouds for Language-Guided Robotic Manipulation**
[[arxiv](https://arxiv.org/abs/2309.15596)]
[[note](PolarNet.html)]

**ManiCM: Real-time 3D Diffusion Policy via Consistency Model for Robotic Manipulation**
[[arxiv](https://arxiv.org/abs/2406.01586)]
[[note](ManiCM.html)]

**SUGAR: Pre-training 3D Visual Representations for Robotics**
[[arxiv](https://arxiv.org/abs/2404.01491)]
[[note](SUGAR.html)]

**SPA: 3D Spatial-Awareness Enables Effective Embodied Representation**
[[arxiv](https://arxiv.org/abs/2410.08208)]
[[note](SPA.html)]

**RVT: Robotic View Transformer for 3D Object Manipulation**
[[arxiv](https://arxiv.org/abs/2306.14896)]
[[note](RVT.html)]

**RVT-2: Learning Precise Manipulation from Few Demonstrations**
[[arxiv](https://arxiv.org/abs/2406.08545)]
[[note](RVT2.html)]

-------

**3D-VLA: A 3D Vision-Language-Action Generative World Model**
[[arxiv](https://arxiv.org/abs/2403.09631)]
[[note](3DVLA.html)]

**AffordDP: Generalizable Diffusion Policy with Transferable Affordance**
[[arxiv](https://arxiv.org/abs/2412.03142)]
[[note](AffordDP.html)]

**Beyond Sight: Finetuning Generalist Robot Policies with Heterogeneous Sensors via Language Grounding**
[[arxiv](https://arxiv.org/abs/2501.04693)]
[[note](BS.html)]

**CogACT: A Foundational Vision-Language-Action Model for Synergizing Cognition and Action in Robotic Manipulation**
[[arxiv](https://arxiv.org/abs/2411.19650)]
[[note](CogACT.html)]

**DeeR-VLA: Dynamic Inference of Multimodal Large Language Models for Efficient Robot Execution**
[[arxiv](https://arxiv.org/abs/2411.02359)]
[[note](DeeRVLA.html)]

**DexVLA: Vision-Language Model with Plug-In Diffusion Expert for General Robot Control**
[[arxiv](https://arxiv.org/abs/2502.05855)]
[[note](DexVLA.html)]

**FAST: Efficient Action Tokenization for Vision-Language-Action Models**
[[arxiv](https://arxiv.org/abs/2501.09747)]
[[note](FAST.html)]

**Improving Vision-Language-Action Models via Chain-of-Affordance**
[[arxiv](https://arxiv.org/abs/2412.20451)]
[[note](CoA.html)]

**Diffusion-VLA: Scaling Robot Foundation Models via Unified Diffusion and Autoregression**
[[arxiv](https://arxiv.org/abs/2412.03293)]
[[note](DiVLA.html)]

**RDT-1B: a Diffusion Foundation Model for Bimanual Manipulation**
[[arxiv](https://arxiv.org/abs/2410.07864)]
[[note](RDT1B.html)]

**Robo-ABC: Affordance Generalization Beyond Categories via Semantic Correspondence for Robot Manipulation**
[[arxiv](https://arxiv.org/abs/2401.07487)]
[[note](RoboABC.html)]